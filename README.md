# cla-Seminar-explainable-AI-2019
This pages are accompanying the course Seminar Explainable AI given by Andreas Holzinger 2019

Here the students interested in the course Explainable-AI find some additional resources, Note that this is the 2019 version
https://human-centered.ai/seminar-explainable-ai-2019/

Artificial Intelligence (AI) and Machine Learning (ML) allow problem solving automatically without any human intervention. Particularly, the family of Deep Learning (DL) is very successful. In the medical domain, however, it is necessary that a medical expert is able to understand why an algorithm has come to a certain result. Consequently, the field of Explainable AI (xAI) is gaining interest. The growing field of xAI studies transparency and traceability of opaque AI/ML and there are already a variety of methods. For example with layer-wise relevance propagation relevant parts of inputs to, and representations in, a neural network which caused a result, can be highlighted. This is a first important step to ensure that end users assume responsibility for AI-assisted decision making. Interactive ML (iML) adds the component of human expertise to these processes by enabling them to re-enact and retrace the results (e.g. plausibility checks). Summarizing, Explainable AI deals with the implementation of transparency and traceability of statistical black‚Äêbox machine learning methods, particularly deep learning (DL), however, in the medical domain there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations.

More information see:
https://onlinelibrary.wiley.com/doi/full/10.1002/widm.1312
